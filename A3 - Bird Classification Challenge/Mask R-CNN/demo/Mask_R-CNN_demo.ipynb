{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN demo\n",
    "\n",
    "This notebook illustrates one possible way of using `maskrcnn_benchmark` for computing predictions on images from an arbitrary URL.\n",
    "\n",
    "Let's start with a few standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import cv2\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this makes our figures bigger\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the relevant imports for the detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.config import cfg\n",
    "from predictor import COCODemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper class `COCODemo`, which loads a model from the config file, and performs pre-processing, model prediction and post-processing for us.\n",
    "\n",
    "We can configure several model options by overriding the config options.\n",
    "In here, we make the model run on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_file = \"../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml\"\n",
    "\n",
    "# update the config options with the config file\n",
    "cfg.merge_from_file(config_file)\n",
    "# manual override some options\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `COCODemo` object. It contains a few extra options for conveniency, such as the confidence threshold for detections to be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coco_demo = COCODemo(\n",
    "    cfg,\n",
    "    min_image_size=800,\n",
    "    confidence_threshold=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few helper functions for loading images from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(url):\n",
    "    \"\"\"\n",
    "    Given an url of an image, downloads the image and\n",
    "    returns a PIL image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    # convert to BGR format\n",
    "    image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return image\n",
    "\n",
    "def imshow(img):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load an image from the COCO dataset. It's reference is in the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from http://cocodataset.org/#explore?id=345434\n",
    "path = \"/home/amine/Documents/3A MVA/Semestre 1/Object Recognition and Computer Vision/HW3/bird_dataset/train_images/021.Eastern_Towhee\"\n",
    "# image = load(\"http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg\")\n",
    "image = cv2.imread(path+\"/Eastern_Towhee_0117_22741.jpg\")\n",
    "# cv2.imshow('Bird example',image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the predictions\n",
    "\n",
    "We provide a `run_on_opencv_image` function, which takes an image as it was loaded by OpenCV (in `BGR` format), and computes the predictions on them, returning an image with the predictions overlayed on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eq() received an invalid combination of arguments - got (str), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mstr\u001b[0m)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mstr\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3ab7c3449b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# imshow(predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcropImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoco_demo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-ab7d05551abf>\u001b[0m in \u001b[0;36mcropImage\u001b[0;34m(image, cocomodel)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboundingRect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bird'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0midx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnew_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eq() received an invalid combination of arguments - got (str), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mstr\u001b[0m)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mstr\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# compute predictions\n",
    "coco_demo = COCODemo(\n",
    "    cfg,\n",
    "    min_image_size=800,\n",
    "    confidence_threshold=0.7,\n",
    ")\n",
    "# predictions = coco_demo.run_on_opencv_image(image)\n",
    "# imshow(predictions)\n",
    "# plt.show()\n",
    "cropImage(image, coco_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cropImage(image, cocomodel):\n",
    "    predictions = cocomodel.compute_prediction(image)\n",
    "    top_predictions = cocomodel.select_top_predictions(predictions)\n",
    "    result = image.copy()\n",
    "    masks = top_predictions.get_field(\"mask\").numpy()\n",
    "    labels = top_predictions.get_field(\"labels\")\n",
    "    colors = cocomodel.compute_colors_for_labels(labels).tolist()\n",
    "    contours = None\n",
    "    for mask, color in zip(masks, colors):\n",
    "        thresh = mask[0, :, :, None]\n",
    "        _, contours, hierarchy = cv2.findContours(\n",
    "            thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        image = cv2.drawContours(result, contours, -1, color, 3)\n",
    "    idx=0\n",
    "    if contours==None:\n",
    "        return None\n",
    "    for i,c in enumerate(contours):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w>50 and h>50:\n",
    "            idx+=1\n",
    "            new_img=image[y:y+h,x:x+w]\n",
    "            cv2.imshow('Mask R-CNN example',new_img)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "            return new_img\n",
    "    \n",
    "#             cv2.imshow('image', new_img)\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "# from http://cocodataset.org/#explore?id=345434\n",
    "# path = \"/home/amine/Documents/3A MVA/Semestre 1/Object Recognition and Computer Vision/HW3/bird_dataset/train_images/021.Eastern_Towhee\"\n",
    "# # image = load(\"http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg\")\n",
    "# image = cv2.imread(path+\"/Eastern_Towhee_0117_22741.jpg\")\n",
    "# # imshow(image)\n",
    "# # plt.show()\n",
    "# cropImage(image, coco_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "rootDir = \"../../../bird_dataset/\"\n",
    "\n",
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    return r\n",
    "\n",
    "directories = list_files(rootDir)\n",
    "\n",
    "# Generating cropped images for each image in directories\n",
    "for i,p in enumerate(directories):\n",
    "    if i%100==0:\n",
    "        print(i*100/len(directories))\n",
    "    image = cv2.imread(p)\n",
    "    new_img = cropImage(image, coco_demo)\n",
    "    if new_img != None:\n",
    "        cv2.imwrite(p[:len(p)-4]+'_cropped.jpg',new_img)\n",
    "#     print(p[:len(p)-4]+'Z.jpg')\n",
    "#     imshow(image)\n",
    "#     plt.show()\n",
    "#     predictions = coco_demo.compute_prediction(image)\n",
    "#     top_predictions = coco_demo.select_top_predictions(predictions)\n",
    "#     result = image.copy()\n",
    "#     # if self.show_mask_heatmaps:\n",
    "#     #     return self.create_mask_montage(result, top_predictions)\n",
    "#     # result = self.overlay_boxes(result, top_predictions)\n",
    "#     if coco_demo.cfg.MODEL.MASK_ON:\n",
    "#         image = result\n",
    "#         predictions = top_predictions\n",
    "#         masks = predictions.get_field(\"mask\").numpy()\n",
    "#         labels = predictions.get_field(\"labels\")\n",
    "\n",
    "#         colors = coco_demo.compute_colors_for_labels(labels).tolist()\n",
    "\n",
    "#         for mask, color in zip(masks, colors):\n",
    "#             thresh = mask[0, :, :, None]\n",
    "#             _, contours, hierarchy = cv2.findContours(\n",
    "#                 thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "#             )\n",
    "#             # image = cv2.drawContours(image, contours, -1, color, 3)\n",
    "\n",
    "#     composite = image\n",
    "#     plt.imshow(composite)\n",
    "#     plt.show()\n",
    "    \n",
    "#     cv2.imwrite(p[:len(p)-4]+'Z.jpg',composite)\n",
    "#     print(p[:len(p)-4]+'Z.jpg')\n",
    "    # result = self.overlay_class_names(result, top_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
